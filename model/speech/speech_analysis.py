import whisper
import librosa
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import difflib

# Whisper ëª¨ë¸ ë¡œë“œ
whisper_model = whisper.load_model("small")

# ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
def transcribe_audio(audio_path):
    result = whisper_model.transcribe(audio_path)
    return result["text"]

# Librosaë¡œ ìŒì„± íŠ¹ì„± ë¶„ì„
def load_audio(audio_path):
    audio, sr = librosa.load(audio_path, sr=16000)
    return audio, sr

# MFCC íŠ¹ì§• ì¶”ì¶œ
def extract_mfcc(audio, sr):
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    mfccs_mean = np.mean(mfccs.T, axis=0)
    mfccs_std = np.std(mfccs.T, axis=0)
    return mfccs_mean, mfccs_std

# Pitch(ì–µì–‘) íŠ¹ì§• ì¶”ì¶œ
def extract_pitch(audio, sr):
    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)
    pitch_values = pitches[magnitudes > np.median(magnitudes)]
    if len(pitch_values) == 0:
        return np.array([0])
    pitch_mean = np.mean(pitch_values)
    pitch_std = np.std(pitch_values)
    return pitch_mean, pitch_std

# ì†ë„(WPM) ì¶”ì •
def estimate_wpm(audio_path, transcript):
    audio, sr = librosa.load(audio_path, sr=16000)
    audio_duration_sec = librosa.get_duration(y=audio, sr=sr)
    word_count = len(transcript.split())
    wpm = (word_count / audio_duration_sec) * 60
    return wpm

# ë°œìŒ ì •í™•ë„ í‰ê°€
def evaluate_pronunciation(user_text, model_text):
    sequence = difflib.SequenceMatcher(None, user_text, model_text)
    return sequence.ratio()

# í…ìŠ¤íŠ¸ ë¹„êµ ì‹œê°í™”
def show_text_differences(reference, transcript):
    diff = difflib.ndiff(reference.split(), transcript.split())
    print("\n[ì›ë¬¸ vs. ìŒì„± í…ìŠ¤íŠ¸ ë¹„êµ ê²°ê³¼]")
    for d in diff:
        if d.startswith("-"):
            print(f"\033[91m{d}\033[0m")  # ë¹¨ê°„ìƒ‰: ëˆ„ë½ë¨
        elif d.startswith("+"):
            print(f"\033[94m{d}\033[0m")  # íŒŒë€ìƒ‰: ì¶”ê°€ë¨
        elif d.startswith("?"):
            continue
        else:
            print(d)

# ì „ì²´ ìŒì„± ë¶„ì„
def analyze_speech(audio_path, script_path, target_wpm=140):
    # ëŒ€ë³¸ íŒŒì¼ ì½ê¸°
    with open(script_path, "r", encoding="utf-8") as f:
        model_text = f.read().strip()

    # í…ìŠ¤íŠ¸ ë³€í™˜ (Whisper)
    transcript = transcribe_audio(audio_path)
    print(f"\n[STT ë³€í™˜ ê²°ê³¼]\n{transcript}\n")

    # ìŒì„± ë¶„ì„ (Librosa)
    audio, sr = load_audio(audio_path)
    mfcc_mean, mfcc_std = extract_mfcc(audio, sr)
    pitch_mean, pitch_std = extract_pitch(audio, sr)
    wpm = estimate_wpm(audio_path, transcript)
    
    print(f"[ìŒì„± ë¶„ì„ ê²°ê³¼]")
    print(f"MFCC Features (Mean): {mfcc_mean}")
    print(f"MFCC Features (STD): {mfcc_std}")
    print(f"Pitch Features (Mean): {pitch_mean}")
    print(f"Pitch Features (STD): {pitch_std}")
    print(f"Words Per Minute: {wpm:.2f}")
    
    # ë°œìŒ ì •í™•ë„ í‰ê°€
    pronunciation_accuracy = evaluate_pronunciation(transcript, model_text)
    print(f"\n[ë°œìŒ ì •í™•ë„ (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ì¤€)]: {pronunciation_accuracy:.2f}")
    
    # í…ìŠ¤íŠ¸ ë¹„êµ ì‹œê°í™”
    show_text_differences(model_text, transcript)

    # ì¢…í•© í‰ê°€
    print("\n[ë°œí‘œ í‰ê°€]")
    if np.mean(mfcc_mean) > 0.2 and pitch_mean > 70 and wpm > 90 and pronunciation_accuracy > 0.6:
        print("âœ… ë°œìŒ, ì–µì–‘, ì†ë„ ëª¨ë‘ ì˜ ì¡°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤! ë°œí‘œê°€ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.")
    elif np.mean(mfcc_mean) > 0.1:
        print("ğŸ”¶ ë°œìŒì€ ì¢‹ìŠµë‹ˆë‹¤! ì–µì–‘ê³¼ ì†ë„ë¥¼ ë” ì¡°ì •í•˜ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.")
    elif pronunciation_accuracy > 0.5:
        print("ğŸ”¶ ë°œìŒì€ ê´œì°®ì€ í¸ì…ë‹ˆë‹¤! ì–µì–‘ê³¼ ì†ë„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì •í•´ë³´ì„¸ìš”.")
    else:
        print("âŒ ë°œìŒ, ì–µì–‘, ì†ë„ì— ë” ë§ì€ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")

# ì˜ˆì‹œ íŒŒì¼ ê²½ë¡œ
audio_path = "data/pitch_sample.m4a"      # ì‚¬ìš©ì ìŒì„± ì…ë ¥
script_path = "data/pitch_sample_script.txt"     # ë°œí‘œ ëŒ€ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼

analyze_speech(audio_path, script_path)
